{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f35df8-81f1-46fe-802e-c97007cd444f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b2fba-b5ce-498c-9132-1330d4383fd1",
   "metadata": {},
   "source": [
    "Current best submission\n",
    "---\n",
    "\n",
    "**Status: 1. Juni 2024, 19:00**\n",
    "\n",
    "*Key: Super voter combination -- best_dev_combination_2024-06-01*\n",
    "\n",
    "*Score: 0.6934 // MPA_ALL_MODALITIES: 0.7184 // MPA_TOP_VIEW: 0.6684*\n",
    "\n",
    "\n",
    "## Model combination\n",
    "\n",
    "\n",
    "\n",
    "|country_id | count | street | evaluation key | based on |dev set|\n",
    "|---|---|---|---|---|---|\n",
    "|QCD| 2089| yes |finetune_QCD_2| topview_streetview_05-27_A |0.7236|\n",
    "|HUN| 78| yes |finetune_QCD_2| topview_streetview_05-27_A |0.4400|\n",
    "|FMW| 1428| no |finetune_FMW|topmodal_swin_05-24_A|TODO|\n",
    "|PNN| 934| no |finetune_PNN|topmodal_swin_05-24_A |TODO|\n",
    "\n",
    "\n",
    "## X-Multi-Modal Late Fusion\n",
    "-----\n",
    "\n",
    "We create a model that can process all three modalities. The backbones are Swin transformers from the TIMM collection, pretrained on ImageNet-1K.\n",
    "\n",
    "- The pretrained transformers provide embeddings for each modality.\n",
    "- These are combined through an attention module (mode: attention2).\n",
    "- At inference time, the model works even when the streetview image is not present.\n",
    "\n",
    "This model can be applied as a one-size-fits-all solution with satisfying performance on the development set. But for the purpose of this challenge, it proved better to train different models\n",
    "\n",
    "### For samples with all modalities\n",
    "\n",
    "For about half the test set, streetview images are available. The multi-modal model uses the following modalities and backbones:\n",
    "\n",
    "- Orthophotos: SwinV2 transformer (base)\n",
    "- Street photos: SwinV2 transformer (base)\n",
    "\n",
    "Link to the pretrained backbone: https://huggingface.co/timm/swinv2_base_window12to24_192to384.ms_in22k_ft_in1k\n",
    "\n",
    "### For samples with only top modalities\n",
    "\n",
    "For the other test set samples, we use the following modalities and backbones:\n",
    "\n",
    "- Orthophotos: SwinV2 transformer (small)\n",
    "- Sentinel-2 data: SwinV2 transformer (small)\n",
    "\n",
    "Link to the pretrained backbone: https://huggingface.co/timm/swinv2_small_window16_256.ms_in1k\n",
    "\n",
    "\n",
    "Data description\n",
    "---\n",
    "\n",
    "- ~All samples from HUN are excluded. They are underrepresented in the test set, and proved very hard to predict even with overfitting on a single batch.~\n",
    "- 5-fold cross validation from StratifiedGroupKFold (stratify on class labels, group cities)\n",
    "- Images are resized to fit the pretrained model\n",
    "  - Orthophotos: Resize (since they all are the same resolution)\n",
    "  - Streetview: RandomResizeAndCrop (since the views and resolution vary)\n",
    "  - Sentinel-2: Patch is created with 4 64x64 images with 3 channels each\n",
    "- Data augmentation:\n",
    "  - Orthophotos: Random horizontal flips and vertical flips with p=0.5; Color jitter (brightness, contrast, and saturation)\n",
    "  - Streetview: Random horizontal flips with p=0.5; Color jitter\n",
    "  - Sentinel-2: Color jitter\n",
    "  \n",
    "### Training procedure\n",
    "---\n",
    "\n",
    "#### Fine-tune to multi-modal data\n",
    "\n",
    "- Early stopping is applied on the validation set confusion matrix diagonal mean.\n",
    "- Train on full node (2 GPUs).\n",
    "- Cross-entropy loss weighted with class weights from inverse sample count\n",
    "\n",
    "#### Fine-tune to country\n",
    "\n",
    "- The trained models are refined further on the individual countries\n",
    "- Performed for QCD, PNN, FMW separately\n",
    "- 5-fold cross validation\n",
    "- QCD: 5 checkpoints x 5-fold CV: Train 25 models\n",
    "- PNN, FMW: 5 checkpoints :: 1-fold CV: Train 5 models\n",
    "\n",
    "\n",
    "Test predictions and post-processing\n",
    "---\n",
    "\n",
    "Test set predictions are generated for each fold, and majority vote is applied. Where there is a tie, the class with the higher probability is chosen (based on the train set, grouped by country ID).\n",
    "\n",
    "Development set\n",
    "---\n",
    "\n",
    "~All Modalities\n",
    "- Accuracy score: 0.7234\n",
    "- MAP:            0.7033\n",
    "\n",
    "Only Topview Modalities\n",
    "- Accuracy score: 0.6293\n",
    "- MAP:            0.5976~\n",
    "\n",
    "Separate models\n",
    "---\n",
    "\n",
    "- *topmodal_swin_05-24_A* for samples without a streetview image (to beat: 0.6195 DEV)\n",
    "- *topview_streetview_05-27_A_alpha* for samples including a streetview image (to beat: 0.7141 DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404e34c1-1425-4298-8a0c-f30794846ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m      2\u001b[0m best_submission_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../submissions/current_best_model/train/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m best_experiments \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(best_submission_root)\n\u001b[1;32m      4\u001b[0m best_experiments\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "best_submission_root = '../submissions/current_best_model/train/'\n",
    "best_experiments = os.listdir(best_submission_root)\n",
    "best_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dba1d4f-4118-4552-8552-9b2fbd32d512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = os.listdir(os.path.join(best_submission_root, best_experiments[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39e173ea-6422-4a91-bba8-ce058e9181e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(best_submission_root, best_experiments[0], runs[0], 'config_tree.log')) as f:\n",
    "    config_tree = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291ca746-f879-4461-b08c-bc6f941964fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONFIG\\n',\n",
       " '├── datamodule\\n',\n",
       " '│   └── _target_: ai4eo_mapyourcity.datamodules.mapyourcity_datamodule.MapYourCi\\n',\n",
       " '│       batch_size: 16                                                          \\n',\n",
       " '│       num_workers: 8                                                          \\n',\n",
       " '│       pin_memory: false                                                       \\n',\n",
       " '│       dataset_options:                                                        \\n',\n",
       " '│         transform: default                                                    \\n',\n",
       " '│         data_dir: /work/ka1176/caroline/gitlab/AI4EO-MapYourCity/scripts/../da\\n',\n",
       " '│         fold: 1                                                               \\n',\n",
       " '│         fold_dir: /work/ka1176/caroline/gitlab/AI4EO-MapYourCity/scripts/../da\\n',\n",
       " '│         fold_key: random_stratified_labels_cities_noHUN                       \\n',\n",
       " '│         model_id:                                                             \\n',\n",
       " '│           topview: swinv2_small_window16_256.ms_in1k                          \\n',\n",
       " '│           streetview: swinv2_small_window16_256.ms_in1k                       \\n',\n",
       " '│           sentinel2: swinv2_small_window16_256.ms_in1k                        \\n',\n",
       " '│         dataset_options_topview:                                              \\n',\n",
       " '│           transform: resize                                                   \\n',\n",
       " '│           img_file: orthophoto.tif                                            \\n',\n",
       " '│           input_size: default                                                 \\n',\n",
       " '│           model_id: swinv2_small_window16_256.ms_in1k                         \\n',\n",
       " '│         dataset_options_streetview:                                           \\n',\n",
       " '│           transform: default                                                  \\n',\n",
       " '│           img_file: street.jpg                                                \\n',\n",
       " '│           input_size: default                                                 \\n',\n",
       " '│           model_id: swinv2_small_window16_256.ms_in1k                         \\n',\n",
       " '│         dataset_options_sentinel2:                                            \\n',\n",
       " '│           transform: patch                                                    \\n',\n",
       " '│           img_file: s2_l2a.tif                                                \\n',\n",
       " '│           input_size: default                                                 \\n',\n",
       " '│           model_id: swinv2_small_window16_256.ms_in1k                         \\n',\n",
       " '│                                                                               \\n',\n",
       " '├── model\\n',\n",
       " '│   └── _target_: ai4eo_mapyourcity.models.mapyourcity_model.MapYourCityModel   \\n',\n",
       " '│       learning_rate: 1.0e-05                                                  \\n',\n",
       " '│       weight_decay: 0.005                                                     \\n',\n",
       " '│       num_classes: 7                                                          \\n',\n",
       " '│       weighted_loss: true                                                     \\n',\n",
       " '│       class_weights:                                                          \\n',\n",
       " \"│         '0': 0.0647                                                           \\n\",\n",
       " \"│         '1': 0.158                                                            \\n\",\n",
       " \"│         '2': 0.16                                                             \\n\",\n",
       " \"│         '3': 0.14                                                             \\n\",\n",
       " \"│         '4': 0.116                                                            \\n\",\n",
       " \"│         '5': 0.143                                                            \\n\",\n",
       " \"│         '6': 0.219                                                            \\n\",\n",
       " '│       validation_metric: confusion_matrix                                     \\n',\n",
       " '│       loss_id: cross_entropy                                                  \\n',\n",
       " '│       drop_modalities:                                                        \\n',\n",
       " '│         topview: 0.0                                                          \\n',\n",
       " '│         sentinel2: 0.0                                                        \\n',\n",
       " '│         streetview: 0.0                                                       \\n',\n",
       " '│       backbone:                                                               \\n',\n",
       " '│         _target_: ai4eo_mapyourcity.models.backbones.TIMMCollectionCombined   \\n',\n",
       " '│         model_id:                                                             \\n',\n",
       " '│           topview: swinv2_small_window16_256.ms_in1k                          \\n',\n",
       " '│           streetview: swinv2_small_window16_256.ms_in1k                       \\n',\n",
       " '│           sentinel2: swinv2_small_window16_256.ms_in1k                        \\n',\n",
       " '│         is_pretrained: true                                                   \\n',\n",
       " '│         num_classes: 7                                                        \\n',\n",
       " '│         out_features:                                                         \\n',\n",
       " '│           topview: 768                                                        \\n',\n",
       " '│           streetview: 768                                                     \\n',\n",
       " '│           sentinel2: 768                                                      \\n',\n",
       " '│         fusion_mode: attention2                                               \\n',\n",
       " '│                                                                               \\n',\n",
       " '├── callbacks\\n',\n",
       " '│   └── model_checkpoint:                                                       \\n',\n",
       " '│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 \\n',\n",
       " '│         monitor: valid_metric                                                 \\n',\n",
       " '│         mode: max                                                             \\n',\n",
       " '│         save_top_k: 1                                                         \\n',\n",
       " '│         save_last: true                                                       \\n',\n",
       " '│         verbose: false                                                        \\n',\n",
       " '│         dirpath: checkpoints/                                                 \\n',\n",
       " '│         filename: epoch_{epoch:03d}                                           \\n',\n",
       " '│         auto_insert_metric_name: false                                        \\n',\n",
       " '│       early_stopping:                                                         \\n',\n",
       " '│         _target_: pytorch_lightning.callbacks.EarlyStopping                   \\n',\n",
       " '│         monitor: valid_metric                                                 \\n',\n",
       " '│         mode: max                                                             \\n',\n",
       " '│         patience: 6                                                           \\n',\n",
       " '│         min_delta: 0                                                          \\n',\n",
       " '│       model_summary:                                                          \\n',\n",
       " '│         _target_: pytorch_lightning.callbacks.RichModelSummary                \\n',\n",
       " '│         max_depth: -1                                                         \\n',\n",
       " '│       rich_progress_bar:                                                      \\n',\n",
       " '│         _target_: pytorch_lightning.callbacks.RichProgressBar                 \\n',\n",
       " '│                                                                               \\n',\n",
       " '├── logger\\n',\n",
       " '│   └── tensorboard:                                                            \\n',\n",
       " '│         _target_: pytorch_lightning.loggers.tensorboard.TensorBoardLogger     \\n',\n",
       " '│         save_dir: /work/ka1176/caroline/gitlab/AI4EO-MapYourCity/scripts/../lo\\n',\n",
       " '│         name: null                                                            \\n',\n",
       " '│         version: multimodal_swin_05-23_B/2024-05-23_14-43-21                  \\n',\n",
       " '│         log_graph: false                                                      \\n',\n",
       " '│         default_hp_metric: false                                              \\n',\n",
       " \"│         prefix: ''                                                            \\n\",\n",
       " '│                                                                               \\n',\n",
       " '├── trainer\\n',\n",
       " '│   └── _target_: pytorch_lightning.Trainer                                     \\n',\n",
       " '│       devices: auto                                                           \\n',\n",
       " '│       min_epochs: 1                                                           \\n',\n",
       " '│       max_epochs: 150                                                         \\n',\n",
       " '│       log_every_n_steps: 50                                                   \\n',\n",
       " '│                                                                               \\n',\n",
       " '├── original_work_dir\\n',\n",
       " '│   └── /work/ka1176/caroline/gitlab/AI4EO-MapYourCity/scripts/..               \\n',\n",
       " '├── data_dir\\n',\n",
       " '│   └── /work/ka1176/caroline/gitlab/AI4EO-MapYourCity/scripts/../data/AI4EO-Map\\n',\n",
       " '├── fold\\n',\n",
       " '│   └── 1                                                                       \\n',\n",
       " '├── fold_dir\\n',\n",
       " '│   └── /work/ka1176/caroline/gitlab/AI4EO-MapYourCity/scripts/../data/AI4EO-Map\\n',\n",
       " '├── ckpt_path\\n',\n",
       " '│   └── None                                                                    \\n',\n",
       " '├── print_config\\n',\n",
       " '│   └── True                                                                    \\n',\n",
       " '├── ignore_warnings\\n',\n",
       " '│   └── True                                                                    \\n',\n",
       " '├── train\\n',\n",
       " '│   └── True                                                                    \\n',\n",
       " '├── test\\n',\n",
       " '│   └── False                                                                   \\n',\n",
       " '├── seed\\n',\n",
       " '│   └── 404                                                                     \\n',\n",
       " '└── name\\n',\n",
       " '    └── multimodal_swin_05-23_B                                                 \\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0596b-98eb-4a14-8fca-a73da5821695",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07fa204-dc15-44e9-a1f5-790eff79cb94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MapYourCity (AI4EO)",
   "language": "python",
   "name": "map-city"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
